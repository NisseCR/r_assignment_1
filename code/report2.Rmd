---
title: "Bike Sharing"
author: "Aga, Karla, Nisse, Ole"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: united
    number_sections: false
    toc: true
    toc_float:
      collapsed: true
    df_print: paged
bibliography: references.bib  
---

<style>
body {
text-align: justify}
</style>

# Abstract
![](../images/bike_sharing.jpeg)  

"*Bikesharing contributes to the advent of more sustainable transportation in cities around the globe.*" [@beland]. Bikesharing programs are designed to provide short-term bicycle rental in stations dispersed throughout the cities and located near public transportation hubs [@beland]. They have numerous environmental and health benefits, such as reducing congestion, complementing other forms of public transportation and encouraging exercise [@beland]. Additionally, accessible bike rental has a lower barrier of entry than purchasing your own bike and is more convenient for out-of-town commuters.  

In principle bicycles pose a good substitute for car use in urban areas, however, they have certain limitations. Among them is the exposure of cyclists to weather while commuting compared to other means of transportation. It would be intuitive for the number of bike rentals to be dependent on current weather conditions. If that is the case, the extent of that relationship would be important information for the bikesharing companies. Potentially, the information could be a factor in variety if business decisions, including level of pricing or supply for bikes in different seasons. Also, bike rental companies might want to know what target group to advertise to. Maybe they can attract casual users to rent their bikes more often, or maybe they want to instead appeal to casual users to become registered users sooner.

Considering these possible applications, this report will attempt to answer: **To what extent can weather data predict the likelihood for the ratio of the usage of bike of registered users to casual users to increase past its average, and is this effect different for working days than for weekend days?** Weather data is understood in terms of temperature, wind speed, humidity and occurrence of weather phenomena (including storms, snow and rain).

This research question sounds a bit complicated, so let us elaborate.
First of all, why not just try to find out if we can predict the likelihood if, in any condition, a registered user is more likely to use a bike, than a casual user? There is a fundamental problem with phrasing the research question like this; what if, by default, a large proportion of the bike rentals are done by registered users in the first place? Instead, we calculate the overall average proportion of registered to casual users, in order to establish a starting point for the analysis. Then, we see if there are any conditions under which this proportion is likely to skew towards registered users.
Then, we predict that this effect is moderated by whether it is a working day, or a weekend day. After all, a registered user might feel more inclined to make use of their subscription when going to work, even when the weather is bad, whereas a non-registered user might instead opt to use public transport, or other forms of transport.

---
# Dataset
The dataset was retrieved from the open-source machine learning repository [UC Irvine](https://archive.ics.uci.edu/dataset/275/bike+sharing+dataset) and Hadi Fanaee-T from the Laboratory of Artificial Intelligence and Decision Support (LIAAD), University of Porto is credited as its author. The bike rentals data originates from the Capital Bikeshare company based in Washington DC, United States. The dataset contains the hourly and daily counts of bike rentals and weather data in the years 2011 and 2012 in the American capital. We opted to use the hourly data due for modeling, due to the larger sample size (the dataset consists of 17379 data points) and less aggregated data. We found the smaller degree of aggregation important, as weather can change drastically throughout a 24 hour period. In case of further interest or replication of our research analysis, you can trace the link to this dataset at the end of this paper in our [References](#ref)

# Loading data

The repository provided us with three files, namely:  

* `hour.csv`, hourly data of bike rentals.
* `day.csv`, daily data of bike rentals.
* `README.txt`, providing additional metadata about the file and their data.

We first specify our dependencies and read the data from `hour.csv`. We also load `day.csv`, solely for comparison purposes in our [EDA](#eda) phase.  

```{r import dependencies, message = FALSE, warning = FALSE}
library(tidyverse)
library(fastDummies)
library(kableExtra)
library(gridExtra, exclude="combine")
library(lubridate)
library(car)
library(ICC)
library(caret)
```

```{r read data  files}
# Source hourly data for model
source_data <- read.csv("../data/hour.csv", header = TRUE, sep = ",") %>%
  as_tibble()

# Extra daily data for EDA
day_df <- read.csv("../data/hour.csv", header = TRUE, sep = ",") %>%
  as_tibble()

# Clone source
data <- source_data

head(data)
```

We then create a sub-selection of variables that are of interest for our model; these are the control, predictor and outcome variables, as well as, variables necessary for EDA and preprocessing.

```{r variable selection}
data <- select(data,
               dteday,
               hr,
               workingday,
               weathersit, 
               temp, 
               atemp, 
               hum, 
               windspeed, 
               casual,
               registered,
               cnt)
```

## Variables description {#vardesc}

```{r variabe table, echo=FALSE}
# creating a table describing the variables used
variables <- read.csv("../data/variables2.csv", header = TRUE, sep = ";")%>%
  as_tibble()
variables <- variables[1:11, 1:5]

variables %>% 
  kable()%>%
  kable_styling()
```
The table above does not contain a raw outcome variable as of yet. Rather, three variables - namely `casual`, `registered` and `cnt` - are components that will be utilised when creating our outcome variable later on.

The categories of `weathersit` represent the following weather phenomena and their combinations:

1.    Clear, Few clouds, Partly cloudy, Partly cloudy
2.    Mist and Cloudy, Mist and Broken clouds, Mist and Few clouds, Mist
3.    Light Snow, Light Rain and Thunderstorm and Scattered clouds, Light Rain and Scattered clouds
4.    Heavy Rain and Ice Pallets and Thunderstorm and Mist, Snow and Fog


# Preprocessing & EDA {#eda}

## Type checking
Some of our categorical variables (i.e. `workingday`, `weathersit`, `hr`) are interpreted as integers rather than factors. We transform these data to factors for ease of processing and modeling.

```{r transform types}
# Factorise columns
data <- data %>%
  mutate(
    workingday = as.factor(workingday),
    weathersit = as.factor(weathersit),
    hr = as.factor(hr)
  )
```

## Creating Additional Variables {.tabset}
This section describes the process of creating new variables that are either necessary for EDA, or form the outcome variable for our logistic model.

### Create outcome variable
By combining the component outcome variables mentioned in [Variable description](#vardesc), we create `registered_prop`, which is the proportion subscribed rentals compared to the total rental count. Consequently, we use the mean of this variable to create a binary outcome variable called `more_registered`.

```{r create outcome variable}
# Create binary variable, returning true if proportion is higher than respective mean.
create_ouctome <- function(prop, mean) {
    ifelse(prop > mean, 1, 0) %>%
    as.factor()
}

# Bind binary variable to data.
data <- data %>%
  mutate(
    registered_prop = registered / cnt,
    more_registered = create_ouctome(registered_prop, mean(registered_prop))
  )

# Inspect data.
head(data)

prop_mean = mean(data$registered_prop)
```

### Expand `dteday` data
We  create a new variable `dt_num` that provides a numeric value that represents an identifier that maps an hourly based entry to a day ID $[1..7]$. We create it by converting the `dteday` variable.

```{r date expansion}
# Converts datetime string to numeric with origin offset.
date_to_num <- function(date, offset) {
   as.integer(as.Date(date, "%Y-%m-%d")) - offset
}

# Fetch origin.
dt_offset <- source_data$dteday[1] %>%
  date_to_num(0) - 1


data <- data %>%
  mutate(
    dt_num = date_to_num(dteday, dt_offset)
  )

# Global variable for amount of unique days.
n_days <- n_distinct(data$dteday)
```

## Descriptive Statistics

We obtained the descriptive statistics of our variables. 

```{r summarise data, echo=FALSE}
summary(data)
```

The most important information we obtained here is that the data contains `r n_days` unique days, meaning that the entirety of 2011 (365 days) and 2012 (366 days) was recorded. However, we note that some `hr` categories occur less often than others within the dataset. This might suggest that entire entries might be missing.

## Distributions {#dists}

Since the summaries are not giving a precise enough picture, we histograms of the variables.

```{r distributions, echo=FALSE, message=FALSE}
grid.arrange(ncol = 3,
    ggplot(data, aes(registered_prop)) + geom_histogram() + 
      labs (x = "Registered proportion", y = "Frequency") + theme_bw(),
    ggplot(data, aes(cnt)) + geom_histogram() + 
      labs (x = "Bike rentals total", y = "Frequency") + theme_bw(),
    ggplot(data, aes(casual)) + geom_histogram() + 
      labs (x = "Bike rentals casual", y = "Frequency") + theme_bw(),
    ggplot(data, aes(registered)) + geom_histogram() + 
      labs (x = "Bike rentals subscribed", y = "Frequency") + theme_bw(),
    ggplot(source_data, aes(temp)) + geom_histogram() + 
      labs (x = "Temperature", y = "Frequency")+ theme_bw(),
    ggplot(data, aes(atemp)) + geom_histogram() + labs (x = "Feeling temperature",
          y = "Frequency")+ theme_bw(),
    ggplot(data, aes(hum)) + geom_histogram() + labs (x = "Humidity",
          y = "Frequency")+ theme_bw(),
    ggplot(data, aes(windspeed)) + geom_histogram() + labs (x = "Wind speed",
          y = "Frequency")+ theme_bw(),
    ggplot(data, aes(weathersit)) + geom_bar(stat = 'count')
      + labs(x = "Weathersit", y = "Frequency") + theme_bw()
)
```

The histogram of the registered proportion reveals that registered users make up the majority of our total rental counts within our data, with the distribution being heavily left-skewed. This confirms why simply comparing the amount of casual vs. registered users will result in a heavily imbalanced binary outcome variable, hence basing the outcome variable based on the average proportion is a more feasible outcome.

The histogram of number of bike rental reveals that lower numbers of rentals are much more frequent, with no (or little to no) bikes being rented being the most frequent state.

The weather phenomena histograms reveals that the 4th category, the harshest weather phenomena, is barely present. We will inspect this further when detecting possible outliers.

The distribution of temperature and feeling temperature is quite normal. However, both humidity and wind speed have a number of `0` values that are disjointed from the rest of the data. They may constitute possible outliers and we will look at them closer in the next section. 

## Outliers {.tabset}

Since there was an indication of outliers, we decided to detect them using quartiles. We made the following box plots:  
```{r boxplots, echo=FALSE, message=FALSE}
grid.arrange(
  ggplot(data, aes(temp)) + geom_boxplot() + labs (x = "Temperature") + theme_bw(),
  ggplot(data, aes(atemp)) + geom_boxplot() + labs (x = "Feeling temperature") + theme_bw(),
  ggplot(data, aes(hum)) + geom_boxplot() + labs (x = "Humidity") + theme_bw(),
  ggplot(data, aes(windspeed)) + geom_boxplot() + labs (x = "Wind speed") + theme_bw(),
  ggplot(data, aes(cnt)) + geom_boxplot() + labs (x = "Bike rentals total") + theme_bw(),
  ggplot(data, aes(casual)) + geom_boxplot() + labs (x = "Bike rentals casual") + theme_bw(),
  ggplot(data, aes(registered)) + geom_boxplot() + labs (x = "Bike rentals subscribed") + theme_bw()
)
```

The box plots further support our observations from earlier. Temperature and feeling temperature are distributed very normally and display no outliers on the box plots. Humidity has a a few `0` values that are flagged as outliers. In the wind speed, the large number of `0` values skews the distribution towards the smaller values, leaving a lot of the high wind speeds as outliers. The box plot of number of bike rentals reveals a large number of outliers in our outcome component variable. A similar pattern can be observed for the casual and registered bike rentals.

We investigate the "problematic" variables: humidity, wind speed and number of rentals, further below:

### Rental count outliers
```{r hour influence}
data %>%
  group_by(hr) %>%
  summarise(
    cnt = sum(cnt)
  ) %>%
  ggplot(aes(hr, cnt)) +
    geom_bar(stat = "identity") +
    labs (x = "Hour (0 represents 1 am)",
          y = "Sum of rentals in 2011 and 2012",
          title = "Distribution of bike rentals throughout the day") +
  theme_bw()
  
```

Looking at the above graph, the high occurrence of lower bike rentals (both registered and casual) is most likely caused by the night hours. Specifically, people barely rent bikes during the night.

In the histogram we see the drastically different night distribution, with the frequency of small numbers of rentals dominating the entire data. However, the distribution of rental counts is quite positively-skewed. Therefore, the data points at the tail-end of the distribution will be marked as outliers within the boxplot. However, the outlier values are quite reasonable. It is realistic for the rental company to be far more likely to receive  smaller number of rentals and not operating at their full capacity. Hence the outliers remain in the dataset.

### Windspeed outliers

The distribution of wind speed is right-skewed, hence large values are seen as outliers. We do note the presence of gap between `0` values and the first non-zero values, specifically:  
```{r windspeed value frequencies}
data %>%
  group_by(windspeed) %>%
  summarise(n = n()) %>%
  arrange(windspeed) %>%
  head()
```

While the `0.00` values fall within the distribution, it is only odd that there's a small increment between the `0.00` occurrences and the values thereafter (e.g. `0.0869`).
This might be due to sensor threshold for measuring wind speed or these might also be possible missing values which were replaced with `0.00`. Since we cannot know for certain and the number of occurrences for `0.00` is in line with neigbouring values, we choose to keep them.

### Humidity outliers

A similar gap between `0.00` values and the next non-zero value occurs in the humidity variable: 
```{r humidity value frequencies}
data %>%
  group_by(hum) %>%
  summarise(n = n()) %>%
  arrange(hum) %>%
  head()
```

In fact, there are 22 hours in which the value is registered. Humidity of `0.00` is not possible in nature, which suggests that the values might be a mistake or a missing value. However, the values have been normalized and we do not know the specific method used. Thus, there is a significant possibility that the value `0.00` does not represent a true `0.00` humidity and is just the minimal value in the data. Since the number of these outliers is small and they could be real data, we chose to keep them.

### Weathersit outliers
```{r weathersit 4 detection}
# Inspect occurrence count for weathersit categories.
data %>%
  count(weathersit)
```

```{r weathersit 4 removal}
# Filter weathersit 4 outliers.
data <- data %>%
  filter(weathersit != 4)
```

## Missing data

The dataset's source claimed there is no missing data. We verified that, by checking if any standard NA values are present.

```{r missing standard}
anyNA(data)
```

There is no standard missing data. There is no missing data in general under any other name. 

### Missing records for hour

The hour graph confirms the missing records in some `hr` categories. We suspect that this could be caused by data cleaning from the author of the dataset, since the night hours are likely to have no rentals. Alternatively, maybe the bike rental company was not operating during those low traffic hours for reasons that could include lack of night workers or updates in the system. Lastly, we considered time changes, however, those would only impact the records of one of the hours. Nevertheless, the difference should not impact the final results of our analysis. 

As mentioned before the only type of missing data is the fact that our dataset is not complete in terms of containing all the hours of the two years. Some `hr` values seem to have a lower amount of occurrences than others, as can be seen below:

```{r number of missing entries, echo=FALSE}

# Count the amount of records per hour.
# Also add a column for amount of *missing* records.
hr_df <- data %>%
  group_by(hr) %>%
  summarise(
    n = n(),
    n_missing_days = n_days - n) # using global variable `n_days`.

ggplot(hr_df, aes(hr, n_missing_days)) +
  geom_bar(stat = "identity")+
  labs (x = "hour, with 0 representing 1 am",
        y =  "Number of missing records",
        title = "Distribution of missing hours")+
  theme_bw()
```

Thus, most of the missing hours are between 3am and 6am. The lack of records during the night could be potentically caused by maintenance conducted by the rental company at those hours, time changes or shortage of night workers. To investigate further we decided to see during which days are the missing hours present: 

*Table showing days with the highest number of missing records*
```{r index of missing entries, message = FALSE, echo=FALSE}
# Create dataframe with amount of missing records per day.
missing_df <- data %>%
  group_by(dteday, dt_num) %>%
  summarise(
    n_records = n(),
    n_missing_records = 24 - n_records
  ) %>%
  filter(n_missing_records > 0) %>%
  arrange(desc(n_missing_records)) %>%
  select(dteday, n_missing_records)

# `n_missing_records` is the amount of missing entries on that particular day.

head(missing_df, 10)
```

The majority of the missing records seems to be concentrated on just four days. Particularly, on the `2012-10-29` and `2012-10-30` the records are missing for 36 consecutive hours, which suggests a system outage of the rental company or another similar disturbance to the measurement of the data. Since the number of missing records is small compared to the size of the dataset, we do not think they will impact our analysis.

## Relations {#relation}

### Covariance amongst predictors
The feeling temperature, represented as `atemp` within the data, by definition might be a combination of other predictor variables such as `temp`, `windspeed` and `hum`. To check whether indedepent variables might correlate (multicollinearity), we scatter plot `atemp` in particular with other predictor variables to observe potential covariance.  

```{r atemp covariances, echo=FALSE, message=FALSE}
grid.arrange(
  ggplot(data, aes(temp, atemp)) + geom_point() + labs(x = "Temperature", y = "Feeling temperature"),
  ggplot(data, aes(windspeed, atemp)) + geom_point() + labs(x = "Wind speed", y = "Feeling temperature"),
  ggplot(data, aes(hum, atemp)) + geom_point() + labs(x = "Humidity", y = "Feeling temperature"),
  nrow = 1
)
```

From this we can conclude that `atemp` and `temp` do in fact covary - showing a strong linear relation, whilst `hum` and `windspeed` show no particular relation with respect to `atemp`.  
Due to the fact that `atemp` might be a correction of `temp` using potential latent variables, we opt to not use the feel temperature in our model.

### Predictor effect on outcome {.tabset}

#### Workingday
Plotting the amount of success and failure occurrences per `workinday` category gives us insight into how the variable possibly affects our outcome. From the bar graph, we can conclude that the proportion of registered users compared to the total amount of users changes dramatically between non-workingdays and workdays. In particular, the proportion of registered users sits below its respective average during non-workingdays, whilst the majority of observations during workdays have a registered proportion higher than its average.
```{r effect workingday}
ggplot(data, aes(workingday)) +
  geom_bar(aes(fill = more_registered))
```

By inspecting the average proportion values per `workingday` category, we can indeed conclude that the proportion is more likely to exceed its respective mean threshold when `workinday` is set to `1`.

```{r change workinday vs. outcome}

data %>%
  group_by(workingday) %>%
  summarise(
    average_prop = mean(registered_prop),
    threshold = prop_mean
  )
```

Thus, `workinday` can possibly have a strong effect on our outcome variable.

#### Temperature
By inspecting the distribution of temperature faceted on our outcome variable, we notice that in case of lower proportions of our registered users, the temperature seems to be left-skewed, whilst higher than usual registered proportions skew the distribution to the right. This could indicate that registered users are more inclined to still rent a bike at lower temperatures compared to casual users, which seems likely.  
Therefore, the outcome variable seems to be affected by `temp` as well.

```{r effect temperature, message = FALSE}
ggplot(data, aes(temp)) +
  geom_histogram(aes(fill = more_registered)) +
  facet_wrap(~ more_registered)
```

#### Humidity
Similar to the variable `temp`, we notice a change in distribution of `hum` based on our outcome variable. In particular, the distribution of `hum` is normally distributed when the proportion of registered users is lower than usual. In the case of the proportion being higher, the `hum` distribution seems to be left-skewed, albeit slightly. This could suggest that registered users are more inclined to go by bike than casual users when the humidity is high. That said, it is worthy to note that the shift in distribution - and therefore the effect of `hum` of our outcome - might weaker compared to that of `temp`, hence the effect of `hum` on our outcome might be weaker than `temp`.

```{r effect humidity, message = FALSE}
ggplot(data, aes(hum)) +
  geom_histogram(aes(fill = more_registered)) +
  facet_wrap(~ more_registered)
```

#### Windspeed
In the case of `windspeed`, the shape of the distribution faceted on our outcome variable does not exhibit a clear change. This is especially true if we compare this to the earlier mentioned predictors, such as `temp`, `hum` and `workingday`. We do observe a change in the total count of occurrences windspeed values when our outcome is true, that said this is probably caused by a slight imbalanced outcome rather.  
Thus, the effect of `windspeed` on our outcome variable could possibly be weak.

```{r effect windspeed, message = FALSE}
ggplot(data, aes(windspeed)) +
  geom_histogram(aes(fill = more_registered)) +
  facet_wrap(~ more_registered)
```

#### Weathersit
When looking at the `weathersit` categories - grouped on our outcome variable - we observe that ratio of success to failures, and thus the registered proportion being larger than usual, is noticeably higher the move severe `weathersit` categories are within our data entries. Specifically, for the severe weather category `3` we could possibly conclude that registered users are far more likely to still go by bike compared to casual users. Thus, `weathersit` might have some effect on our outcome variable and is hence worth building our model around.

```{r effect weathersit, message = FALSE}
ggplot(data, aes(weathersit, fill = more_registered)) +
  geom_bar()
```

# Model selection - Phase 1 {#create_model}
Our approach to modeling was to build up nested models using the step-up strategy [@Grace-Martin_2023] - starting with an empty intercept-model - and adding potential predictors in a step-wise manner. The order in which these predictors were added was based on the previous analysis done in [Relations](#relations), starting with variables that seemed to affect the outcome variable the most. From said analysis, we concluded that `workingday` had the strongest effect on our outcome variable - hence this was the first predictor that was to be added. Building on top of that, we added the remaining variables one by one in order of decreasing effect.  
Logistic regression models do not have an $R^2$ statistic. Instead, we used the *residual deviance* $D$ in order to compare model fit improvement between models. The significance of this improvement was validated using *likelihood ratio tests* between two nested (adjacent) models. To also take into account the increase of model complexity when adding in predictors, we utilised the *AIC* and *BIC* measures in our comparison process.  
A total of 6 non-moderated models were created, all of which were an improvement compared to their parent version. Consequently, model 5 (zero-based) was identified as our best model based on the *AIC*, *BIC* and $D$ statistics. This model includes all the weather predictors, as well as the `workingday` indicator.  
Lastly, we checked whether `workinday` moderated the relation between our weather predictors and our outcome variable by creating our last model - model 6. We tested if adding the moderation was a significant improvement by once again comparing the *AIC* and *BIC* values between our moderator model and the "best model", whilst also performing a *likelihood ratio test* between the two.  

The entirety of this process is reflected in the code below.  

***

We start by creating the models:

```{r model creation}
# Create logistic models.
model0 <- glm(more_registered ~ 1, family = binomial, data)
model1 <- glm(more_registered ~ workingday, family = binomial, data)
model2 <- glm(more_registered ~ workingday + temp, family = binomial, data)
model3 <- glm(more_registered ~ workingday + temp + hum, family = binomial, data)
model4 <- glm(more_registered ~ workingday + temp + hum + windspeed, family = binomial, data)
model5 <- glm(more_registered ~ workingday + temp + hum + windspeed + weathersit, family = binomial, data)

# Add working day moderator on weather predictors.
model6 <- glm(more_registered ~ workingday*(windspeed + temp + hum) + weathersit, family = binomial, data)
```

Having created the models, we can obtain the required comparison statistics and display them in a table:

```{r model comparison}
# Obtain residual deviance statistic from model.
RD <- function(model) {
  summary(model)$deviance
}

# Create AIC, BIC and D table for all the models.
comp_df <- data.frame(
  model = c('model0', 'model1', 'model2', 'model3', 'model4', 'model5', 'model6'),
  predictors = c('', 'workingday', 'workingday + temp', 'workingday + temp + hum', 'workingday + temp + hum + windspeed', 'workingday + temp + hum + windspeed + weathersit', 'workingday + temp + hum + windspeed + weathersit'),
  moderator = c('', '', '', '', '', '', 'workingday'),
  AIC = c(AIC(model0), AIC(model1), AIC(model2), AIC(model3), AIC(model4), AIC(model5), AIC(model6)),
  BIC = c(BIC(model0), BIC(model1), BIC(model2), BIC(model3), BIC(model4), BIC(model5), BIC(model6)),
  D = c(RD(model0), RD(model1), RD(model2), RD(model3), RD(model4), RD(model5), RD(model6))
)

comp_df %>% 
  kable()%>%
  kable_styling()
```

Based on the model fit statistics in the table, we can conclude that model 5 is the non-moderated model with the lowest *AIC*, *BIC* and $D$ values. In fact, adding in predictors always showed a decrease in these values, albeit diminishing. The latter is especially true for the obtained decreases between model 3, 4 and 5 respectively.  
The addition of the `workingday` moderator clearly improved the model, seeing how the decrease in *AIC*, *BIC* and $D$ values is proportionally large when comparing the improvement between 4 & 5 to 5 & 6.  

Lastly, we perform *likelihood ratio tests* / *anova tests* in order to verify the significance of the $D$ value decreases:

```{r model anova comparison}
# Perform the likelihood ratio tests.
anova(model0, model1, test = "LRT")
anova(model1, model2, test = "LRT")
anova(model2, model3, test = "LRT")
anova(model3, model4, test = "LRT")
anova(model4, model5, test = "LRT")
anova(model5, model6, test = "LRT")
```

The anova tests show that each addition of a predictor resulted in a significant decrease of the *residual deviance*. This also holds for the addition for our moderator `workinday`.  

***

Thus, the final model we chose is model 6 - which includes the predictors `workinday`, `temp`, `hum`, `windspeed`, `weathersit` whilst also having a moderator `workinday` that effects the relation between our weather predictors and the outcome variable.

# Assumptions - Phase 1
## Full-rank predictor matrix

For this assumption to hold, there need to be more observations within the data set than predictors in the model. This assumption is easily met - there are 17376 observations, whilst the model uses 8 predictors.  

Furthermore, the model should not suffer from major multicollinearity. If multicollinearity is present, it can inflate the variance present within a model. Moreover, it prevents a model from uniquely determining parameter estimates from a model, therefore making the model not estimable.  
We appeal to the variance inflation factor or *VIF*, in order to check whether we are dealing with multicollinearity between our predictors. 

```{r vif check 1, message = FALSE}
vif(model6)
```
From this we can conclude that the interaction terms between our moderator and weather predictors show severe multicollinearity, as is indicated by the GFIV values surpassing the $> 10$ threshold. Although this supposedly violates the assumption, it is worth to consider that it makes sense that the interaction terms highly correlate with their respective variables [@interaction].  
To combat the issue of supposed multicollinearity according to the *VIF* values, we chose to mean-center our data [@wiley]. Mean-centering makes no practical difference as to what the regression equation conveys in moderation analysis, moreover it has no effect on model statistics [@gurnsey]. That said, the interpretation of our conditionals change - as other variables are considered to be their mean value rather than `0`.

# Model selection - Phase 2
## Data transformation
We apply mean-centering as a default solution to  the high *VIF* values for our interaction terms. The transformation is done only on our continuous variables.
```{r apply mean-centering}
# Mean-centers the data without applying scaling.
center_scale <- function(x) {
    scale(x, scale = FALSE)
}

# Transform the data.
data <- data %>%
  mutate(across(where(is.numeric), center_scale))

# Inspect the data
summary(data)
```
Inspecting the transformed data, we can see that the mean values of the continuous variables sit at `0`, whilst their min-max range stays the same as a consequence of not scaling the data.

## Remodel with transformation
Using the updated data, we recreate the same sequence of nested models
```{r remodel creation}
# Create logistic models.
model0 <- glm(more_registered ~ 1, family = binomial, data)
model1 <- glm(more_registered ~ workingday, family = binomial, data)
model2 <- glm(more_registered ~ workingday + temp, family = binomial, data)
model3 <- glm(more_registered ~ workingday + temp + hum, family = binomial, data)
model4 <- glm(more_registered ~ workingday + temp + hum + windspeed, family = binomial, data)
model5 <- glm(more_registered ~ workingday + temp + hum + windspeed + weathersit, family = binomial, data)

# Add working day moderator on weather predictors.
model6 <- glm(more_registered ~ workingday*(windspeed + temp + hum) + weathersit, family = binomial, data)
```

We conducted thorough inspection in order to validate that the interaction terms and comparison statistics (*AIC*, *BIC*, $D$) were unchanged as a result of the data transformation. Comparing the coefficients and the model statistics, we concluded that this was indeed the case. Due to this, a identical analysis was performed to select our best model - following the same steps as in our previous [model creation](#create_model). This once again resulted in model 6 to be our best fitted model, including the moderator `workinday` on our weather predictor variables.

# Assumptions - Phase 2
## Full-rank predictor matrix
The amount of observations again exceeds the number of predictors in the model.

To check for multicollinearity, we once more appeal to the variance inflation factor or *VIF*.

```{r vif check 2, message = FALSE}
vif(model6)
```
The *VIF* values all predictor variables indicate an absence of severe multicollinearity. This now also holds for our interaction terms as a result of mean-centering our data.

## Linearity
The assumption that the continuous predictors and the logit of the success probability should be linearly related, must hold. If the predictors show a non-linear relation, than the current model paradigm is not suited for the regression analysis.

We first inspect whether the pearson residuals vs. the fitted  values show a linear relation:  

```{r check linearity 1}
plot(model6, 1)
```

This graph indeed suggests a linear relation.

Since the `car` package does not support models with interaction terms, we were unable to utilise its partial residual plots feature. Therefore, we have opted to inspect the relation between the logit values and fitted values *per* predictor:

```{r partial residual plots, message = FALSE}
data$logit <- predict(model6, type = "link")

data %>%
  ggplot(aes(temp, logit)) +
  geom_point() +
  geom_smooth()

data %>%
  ggplot(aes(hum, logit)) +
  geom_point() +
  geom_smooth()

data %>%
  ggplot(aes(windspeed, logit)) +
  geom_point() +
  geom_smooth()
```

Looking at the relation of the weather predictors vs. the logit of the success probability, we can conclude that these also show a linear relation. In case of `hum`, the `0` values do not follow this linear trend. This is perhaps due to them being outliers.

## IID Binomial {#iid}
The outcome of the model should be binary and follow a independent and identically distributed binomial distribution. If this assumption is violated, it might introduce a bias in our model.

For this assumption to hold, we obtain the levels of our outcome variable whilst also showing the ratio of our minority- and majority class.
```{r check binary outcome}
data %>%
  count(more_registered) %>%
  mutate(prop = round(n / sum(n), 2))
```

Our outcome is indeed binary, moreover the outcome is balanced. Whilst the majority class occurs more often, it is nothing too egregious.

Lastly, there needs to be an absence of residual clustering within our data. To validate this absence, we calculate the ICC using the deviance residuals. With the variables provided with the data, we chose to investigate whether the `hr` class causes residual clustering:
```{r check residual clustering}
ICCbare(x = data$hr, y = resid(model6, type = "deviance"))
```
This is not the case, hence the entire assumption holds.

## Balanced outcomes
The accuracy will be affected by an major imbalance in the outcome class. Small imbalances are handled quite well, hence these do not pose an issue in our analysis.  
As noted in our [IID Binomial assumption](#iid), the outcome classes are fairly balanced. The majority class occurs 57% of the time, which we do not observe as a major imbalance.

## Sufficient sample size

Using the proportion of the minority class, obtained from previous [outcome table](#iid), we can calculate the minimal amount of positive cases necessary for this assumption to hold.
```{r check sample size}
minimum_positive_cases <- function(k, p) {
  round((10 * k) / p)
}

minimum_positive_cases(4, 0.43)
```

This assumption is easily met - there are 17376 observations.

## Influential data points
The model should not be contaminated due to extremely influential data points. We can detect influential observations by looking at the leverage of our predictors and whether an outcome value is an outlier - this is done by observing the Cook's Distance of our observations. Furthermore, we plot residuals vs. leverage to gain insight into potential influential data points.  

```{r check infuential outliers}
plot(model6, which = c(4, 5))
```

As we do not observe any data points falling outside of the Cook's Distance threshold, we can conclude thaare are influential observations and thus the assumption holds.

## Deviance residuals

Technically, we don't need to check for deviance residuals as a core assumption for logistic regression. It is, however, a good way to check if the data points are fit well by the model.

```{r check deviance residuals}
dr <- resid(model6, type = "deviance")

eta <- predict(model6, type = "link")

dr_data <- data.frame(residuals = dr, eta = eta)

ggplot(dr_data, aes(x = eta, y = residuals)) +
  geom_point() +
  geom_smooth(se = FALSE)

plot(model6, which = 1)
```

Both regression lines should be roughly flat. Though for the first plot it seems to be a little off, nothing too violating seems to be going on. We consider these regression lines to be sufficient.

# Interpretation of final model

Based on the model comparision earlier, we chose model 6, which is represented by the following formula:$$
    logit(\hat{\pi}) = \hat{\beta_0} +  \hat{\beta_1}workingday + \hat{\beta_2} windspeed + \hat{\beta_3} temp + \hat{\beta_4} hum + \hat{\beta_5} workingday * windspeed + $$
    $$ +\hat{\beta_6} whethersit2 + \hat{\beta_7}whethersit3 + \hat{\beta_8} workingday * temp + \hat{\beta_9} workingday * hum
    $$

## Quality of the model

```{r}
summary(model6)
```

Interpret: 
- is the model significant
- is each predictor significant + interpretation of it's effect

Quality of the model - ROC curve

## Classification Performance

For classification purposes use a threshold of 0.5, as in if the probability of the proportion of registered users is higher than average is 50% then the case is classified as a "success". We decided to use the standard 0.5 threshold because we did not find any substantial reason to justifying changing it to a different value. 

```{r confusion matrix}
# to create a confusion matrix we require a variable that informs us if the model classifies a given 
data <- data %>%
mutate(piHat = predict(model6, type = "response"),
yHat = as.factor(ifelse(piHat <= 0.5, "0", "1"))
)

#creating the confusion matrix 
cMat <- confusionMatrix(data = data$yHat, reference = data$more_registered)
cMat$table

# extracting the Accuracy and other classification statistics for the overall model
cMat$overall 
# extracting the Accuracy and other classification statistics for Positive and Negative prediction
cMat$byClass

```
### Summary of the confusion matrix
Based on the output above we find the following characteristics of our model: 
```{r table_prediction_statistics, echo = FALSE}

# We decided to present the most important predicting characteristics of our model (Accuracy, Sensitivity etc) in a form of a table. This code creates this table

summary_confmatrix <- data.frame(
  Measure = c('Accuracy', 'Error Rate', 'Sensitivity', 'Specificity', 'False Positive Rate', 'Positive Predictive Value', 'Negative Predictive Value'),
  Value = c(round(cMat$overall[1], digits = 3), round((1585 + 2988)/ ( 1585 + 2988 + 4509 + 8297), digits = 3), round(cMat$byClass[1], digits = 3), round(cMat$byClass[2], digits = 3), round(2988 / (5409 + 2988), digits = 3), round(8297 / (8297 + 2988), digits = 3), round(5409 / (5409 + 1585), digits = 3)),
  Interpretation = c('% correctly classified', '% incorrectly classified', '% of cases with registered rentals ratio being above average that are correctly classified', '% of cases with registered rentals ratio being below average that are correctly classified', '% of  cases with registered rentals ratio being below are incorrectly classified as more than average', '% chance that a case classified as above average was classified correctly', '% chance that a case classified as below average was classified correctly'),
  Formula = c('(TP + TN)/ (P + N)', '(FP + FN)/ (P + N)', 'TP / (TP + FN)', 'TN / (TN + FP)', 'FP / (TN + FP)', 'TP / (TP + FP)', 'TN / (TN + FN)')
)

summary_confmatrix %>% 
  kable()%>%
  kable_styling()
```


## Intercept

## Significance of predictors

## Interpretation of coefficents 
- invert the coefficents to exponential

## Answering the research question


# References {#ref}

<div id="refs"></div>

