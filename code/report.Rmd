---
title: "Bike Sharing"
author: "Aga, Karla, Nisse, Ole"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: united
    number_sections: false
    toc: true
    toc_float:
      collapsed: true
    df_print: paged
bibliography: references.bib  
---

<style>
body {
text-align: justify}
</style>

# Abstract
![](../images/bike_sharing.jpeg)  

"*Bikesharing contributes to the advent of more sustainable transportation in cities around the globe.*" [@beland]. Bikesharing programs are designed to provide short-term bicycle rental in stations dispersed throughout the cities and located near public transportation hubs [@beland]. They have numerous environmental and health benefits, such as reducing congestion, complementing other forms of public transportation and encouraging exercise [@beland]. Additionally, accessible bike rental has a lower barrier of entry than purchasing your own bike and is more convenient for out-of-town commuters.  

In principle bicycles pose a good substitute for car use in urban areas, however, they have certain limitations. Among them is the exposure of cyclists to weather while commuting compared to other means of transportation. It would be intuitive for the number of bike rentals to be dependent on current weather conditions. If that is the case, the extent of that relationship would be important information for the bikesharing companies. Potentially, the information could be a factor in variety if business decisions, including level of pricing or supply for bikes in different seasons.   

Considering these possible applications, this report will attempt to answer: **To what extent can weather data predict the number of bike rentals in different parts of the day?** Weather data is understood in terms of temperature, wind speed, humidity and occurrence of weather phenomena (including storms, snow and rain). 

---
# Dataset
The dataset was retrieved from the open-source machine learning repository [UC Irvine](https://archive.ics.uci.edu/dataset/275/bike+sharing+dataset) and Hadi Fanaee-T from the Laboratory of Artificial Intelligence and Decision Support (LIAAD), University of Porto is credited as its author. The bike rentals data originates from the Capital Bikeshare company based in Washington DC, United States. The dataset contains the hourly and daily counts of bike rentals and weather data in the years 2011 and 2012 in the American capital. We opted to use the hourly data due for modeling, due to the larger sample size (the dataset consists of 17379 data points) and less aggregated data. We found the smaller degree of aggregation important, as weather can change drastically throughout a 24 hour period. In case of further interest or replication of our research analysis, you can trace the link to this dataset at the end of this paper in our “References.” 

# Loading data

The repository provided us with three files, namely:  

* `hour.csv`, hourly data of bike rentals.
* `day.csv`, daily data of bike rentals.
* `README.txt`, providing additional metadata about the file and their data.

We first specify our dependencies and read the data from `hour.csv`. We also load `day.csv`, solely for comparison purposes in our [EDA](#eda) phase.  

```{r import, message = FALSE}
library(tidyverse)
library(fastDummies)
library(kableExtra)
library(gridExtra, exclude="combine")
library(lubridate)
library(car)
library(ICC)
```

```{r read bike}
# Source hourly data for model
source_data <- read.csv("../data/hour.csv", header = TRUE, sep = ",") %>%
  as_tibble()

# Extra daily data for EDA
day_df <- read.csv("../data/hour.csv", header = TRUE, sep = ",") %>%
  as_tibble()

# Clone source
data <- source_data

head(data)
```

We then create a sub-selection of variables that are of interest for our model; these are the control, predictor and outcome variables, as well as, variables necessary for EDA and preprocessing.

```{r select}
data <- select(data,
               dteday,
               hr, 
               weathersit, 
               temp, 
               atemp, 
               hum, 
               windspeed, 
               cnt)
```

## Variables description  

```{r var_table, echo=FALSE}
# creating a table describing the variables used
variables <- read.csv("../data/variables2.csv", header = TRUE, sep = ";")%>%
  as_tibble()
variables <- variables[1:8, 1:5]

variables %>% 
  kable()%>%
  kable_styling()

```

The categories of 'weathersit' represent the following weather phenomena and their combinations:

1.    Clear, Few clouds, Partly cloudy, Partly cloudy
2.    Mist and Cloudy, Mist and Broken clouds, Mist and Few clouds, Mist
3.    Light Snow, Light Rain and Thunderstorm and Scattered clouds, Light Rain and Scattered clouds
4.    Heavy Rain and Ice Pallets and Thunderstorm and Mist, Snow and Fog


# Preprocessing & EDA {#eda}

## Creating Additional Variables {.tabset}
We decided that including a time component in our final model is essential. Night hours are approximately half of the data and are characterized by much lower rental counts than the daylight values. Without separating the night data the predictors we expected the predicted rentals during the more active periods to be underestimated. However, we are interested in modeling rental values for the entire day. Thus, we do want to include the hours as a categorical predictor to be able to control the night and day periods. 

However, 24 dummy codes is too much for substantive interpretation. Thus, we decided to aggregate the hourly data into 3 day segments night (23-6), morning-noon(7-14) and eve (15-22). The decision to divide into these categories was made by looking at the distribution of the rental counts at specific hours. We noticed a natural division into these 3 segments. Dividing the time in these periods would potentially solve the problem of interpreting dummy encodings and clusters of  low-night and high-day rentals being incorrectly estimated.

This section describes the process of creating new variables that will be later used to aggregate the data into segments. The aggregation process is described in [Data Aggregation](#agg).

### Expand `dteday` data
We  create a new variable 'dt_num' that provides a numeric value that represents to which out of the 731 days in the dataset a given hourly data point belongs. We create it by converting the `dteday` variable.

```{r expand date}
# Converts datetime string to numeric with origin offset.
date_to_num <- function(date, offset) {
   as.integer(as.Date(date, "%Y-%m-%d")) - offset
}

# Fetch origin.
dt_offset <- source_data$dteday[1] %>%
  date_to_num(0) - 1


data <- data %>%
  mutate(
    dt_num = date_to_num(dteday, dt_offset)
  )

# Global variable for amount of unique days.
n_days <- n_distinct(data$dteday)
```

### Clustering of `hr` categories {#clus}
We create a new variable 'hr_seg' that gives each data point with an hour within our giver segment the name of that segment. 
```{r hour cut}
# Shift the `hr` data by 2 and increment the hour 23 for easier binning via the cut function.
data <- data %>%
  mutate(hr_idx = hr + 2)

data$hr_idx[data$hr == 23] <- 1

# Cut the 24 hour entries in 3 categories.
data <- data %>%
  mutate(hr_seg = cut(
    hr_idx,
    breaks = c(0, 8, 16, 24),
    labels = c("night", "morning-noon", "eve")
  ))
```

After creating the categories, we check whether segmenting of `hr` was applied correctly by looking at single day data:

```{r bin check}
# Check output for single day.
data %>%
  filter(dteday == "2011-01-01") %>%
  arrange(hr) %>%
  select(hr, hr_seg) %>%
  head(24)
```

The segments were chosen based on distribution of rental counts in given hours. Additionally, we wanted the segments to be of equal size to ensure all hours are equally represented in the model. Below we present one of the graphs we used to make the decision. 

```{r determine segment, message=FALSE, echo=FALSE}

# Plot hr~cnt with current segmentation indicator.
data %>%
  group_by(hr, hr_seg) %>%
  summarise(
    cnt = sum(cnt)
  ) %>%
  ggplot(aes(hr, cnt, fill = hr_seg)) +
    geom_bar(stat = "identity")+
    labs (x = "Hour, where 0 reprenents 1 am",
          y = "Sum of all rentals at that hour in 2011 and 2012",
          title = "Distribution of bike rentals throughout the day")+
  theme_bw()
```

## Descriptive Statistics

We obtained the descriptive statistics of our variables. 

```{r summary, echo=FALSE}
summary(data)
```

The most important information we obtained here is that the data contains `r n_days` unique days, meaning that the entirety of 2011 (365 days) and 2012 (366 days) was recorded. However, we note that there is approximately 100 less "night" hours in the dataset than the other two segments suggesting there might be some missing records.  

## Distributions {#dists}

Since the summaries are not giving a precise enough picture, we histograms of the variables.

```{r distributions, echo=FALSE, message=FALSE}
grid.arrange(
      ggplot(data, aes(cnt)) + geom_histogram() +labs (x = "number of bike rentals",
          y = "frequency")+ theme_bw(),
    ggplot(data, aes(hr)) + geom_histogram(binwidth = 1) + labs (x = "hour",
          y = "frequency")+ theme_bw(),
    ggplot(data, aes(weathersit)) + geom_histogram(binwidth = 1) + labs (x = "weather phenomena", y = "frequency")+ theme_bw(),
    ggplot(data, aes(temp)) + geom_histogram() + labs (x = "temperature",
          y = "frequency")+ theme_bw(),
    ggplot(data, aes(atemp)) + geom_histogram() + labs (x = "feeling temperature",
          y = "frequency")+ theme_bw(),
    ggplot(data, aes(hum)) + geom_histogram() + labs (x = "humidity",
          y = "frequency")+ theme_bw(),
    ggplot(data, aes(windspeed)) + geom_histogram() + labs (x = "wind speed",
          y = "frequency")+ theme_bw()
)
```

The histogram of number of bike rental reveals that lower numbers of rentals are much more frequent, with no (or little to no) bikes being rented being the most frequent state. Based on the graph used to create day segments, we are fairly certain that this distribution would look differently for the different segments and majority of the low number of rentals occur in the night. 

The hour graph confirms the missing records in some `hr` categories. We suspect that this could be caused by data cleaning of the author of the dataset, since the night hours are likely to have no rentals. Alternatively, maybe the bike rental company was not operating during those low traffic hours for reasons that could include lack of night workers or updates in the system. Lastly, we considered time changes, however, those would only impact the records of one of the hours. Nevertheless, the difference should not impact the final results of our analysis. 

The weather phenomena histograms reveals that the 4th category, the harshest weather phenomena, is barely present. If we use it in our model the category would be a categorical outlier and needs to be addressed. 

The distribution of temperature and feeling temperature is quite normal. However, both humidity and wind speed have a number of "0" values that are disjointed from the rest of the data. They may constitute possible outliers and we will look at them closer in the next section. 

## Outliers {.tabset}

Since there was an indication of outliers, we decided to detect them using quantiles. The following boxplots with an IQR multipler equal to 1.5 are made:
```{r outliers, echo=FALSE, message=FALSE}
grid.arrange(
    ggplot(data, aes(temp)) + geom_boxplot() + labs (x = "temperature")+ theme_bw(),
    ggplot(data, aes(atemp)) + geom_boxplot() + labs (x = "feeling temperature")+ theme_bw(),
    ggplot(data, aes(hum)) + geom_boxplot() + labs (x = "humidity")+ theme_bw(),
    ggplot(data, aes(windspeed)) + geom_boxplot() + labs (x = "wind speed")+ theme_bw(),
    ggplot(data, aes(cnt)) + geom_boxplot() + labs (x = "number of bike rentals")+ theme_bw()
)
```

The box plots further support our observations from earlier. Temperature and feeling temperature are distributed very normally and display no outliers on the box plots. Humidity has a a few "0" values that are flagged as outliers. In the wind speed, the large number of "0" values skews the distribution towards the smaller values, leaving a lot of the high wind speeds as outliers. The box plot of number of bike rentals reveals a large number of outliers in our outcome variable. As we already saw that the night hours are dominated by small rental number, thus skewing the distribution to the left and leaving the high rental values as outliers. 

We investigate the "problematic" variables: humidity, wind speed and number of rentals, further below:

### Rental count outliers

Since we suspect that a large amount of rentals' outliers is due to low rental count during the night, we decided to make separate histogram and boxplot graphs for the three day segments.
```{r cnt histogram, echo=FALSE, message=FALSE}
ggplot(data, aes(cnt)) + geom_histogram() + facet_wrap(vars(hr_seg))+labs (x = "number of bike rentals",  title = "Distribution of bike rentals throughout the day segments")+ theme_bw()
```

In the histogram we see the drastically different night distribution to the other two day segments, with the frequency of small numbers of rentals dominating the entire data. However, all three distributions are quite positively-skewed even after the division  into the time segments. 

```{r cnt outliers, echo=FALSE, message=FALSE}
ggplot(data, aes(cnt, colour = hr_seg)) + geom_boxplot()+ labs (x = "number of bike rentals",  title = "Boxplot of bike rentals, seperated by the day segments")+ theme_bw()
```

The right-skewed distributions throughout the segments are visible in the boxplots. That is why dividing the boxplot according to the time segments still results in outliers at the tail-end of the values. However, the outlier values are quite reasonable. It is realistic for the rental company to be far more likely to receive  smaller number of rentals and not operating at their full capacity. Hence the outliers remain in the dataset.

### Windspeed outliers

The distribution of wind speed is right-skewed, hence large values are seen as outliers. It is importnat to notice how there's gap between "0" values and the first non-zero values, specifically:  
```{r windspeed outliers}
data %>%
  group_by(windspeed) %>%
  summarise(n = n()) %>%
  arrange(windspeed) %>%
  head()
```

While the "0" values fall within the distribution, it is only odd that there's a small increment between the `0.00` occurrences and the values thereafter (e.g. `0.0869`).
This might be due to sensor threshold for measuring wind speed or these might also be possible missing values which were replaced with `0.00`. Since we cannot know for certain and the number of occurrences for `0.00` is in line with neigbouring values, we choose to keep them.

### Humidity outliers

A similar gap between '0.00' values and the next non-zero value occurs in the humidity variable: 
```{r hum outliers}
data %>%
  group_by(hum) %>%
  summarise(n = n()) %>%
  arrange(hum) %>%
  head()
```

In fact, there is 22 hours in which the value is registered. Humidity of '0.00' is not possible in nature, which suggests that the values might be a mistake or a missing value. However, the values have been normalized and we do not know the specific method used. Thus, there is a significant possibility that the value '0.00' does not represent a true '0.00' humidity and is just the minimal value in the data. Since the number of these outliers is small and they could be real data, we chose to keep them. 

## Missing data

Check standard NA values. There are none.

```{r missing standard}
anyNA(data)
```

### Missing records for hour

Some `hr` values seem to have a lower amount of occurrences than others, as can be seen in distribution plot of [Distributions](#dists)

```{r hr missing}
# Count the amount of records per hour.
# Also add a column for amount of *missing* records.
hr_df <- data %>%
  group_by(hr) %>%
  summarise(
    n = n(),
    n_missing_days = n_days - n) # using global variable `n_days`.

ggplot(hr_df, aes(hr, n_missing_days)) +
  geom_bar(stat = "identity")
```

Determine where missing records are located. Seems that a system outage (or the like) was present on `2012-10-29` and `2012-10-30`, with a combined total of `35` missing records.
```{r missing location, message = FALSE}
# Create dataframe with amount of missing records per day.
missing_df <- data %>%
  group_by(dteday, dt_num) %>%
  summarise(
    n_records = n(),
    n_missing_records = 24 - n_records
  ) %>%
  filter(n_missing_records > 0) %>%
  arrange(desc(n_missing_records)) %>%
  select(dteday, n_missing_records)

head(missing_df, 10)
```

`n_missing_records` is the amount of missing entries on that particular day.

## Covariance {.tabset}

### Covariance with predictor `atemp`

Have suspicion that `atemp` might be correlated with `temp`, `windspeed` and `hum`, since it is a combined variable of the three (mayhaps). Justify this:

```{r atemp covar, echo=FALSE}
grid.arrange(
  ggplot(data, aes(temp, atemp)) + geom_point(),
  ggplot(data, aes(windspeed, atemp)) + geom_point(),
  ggplot(data, aes(hum, atemp)) + geom_point(),
  nrow = 1
)
```

Can conclude that predictor variables `atemp` and `temp` are heavily correlated, whilst `hum` and `windspeed` have a weaker relation with `atemp`.

### Covariance with outcome variable `cnt`

Remark:  

- Non-linear relation between `hum` and `cnt`
- Negative relation between `windspeed` and `cnt`
- Positive relation between `temp` and `cnt`, up until high temperatures are reached (ppl dont like biking in hot water, don't blame them).

```{r cnt covar, echo=FALSE}
plot_cnt_covar <- function(plot) (
  plot
    + geom_point() 
    + geom_smooth(method = 'loess') 
    + facet_wrap(vars(hr_seg))
)

ggplot(data, aes(temp, cnt)) %>% plot_cnt_covar()
ggplot(data, aes(windspeed, cnt)) %>% plot_cnt_covar()
ggplot(data, aes(hum, cnt)) %>% plot_cnt_covar()

```

## Data Aggregation {#agg}
After having created `hr_seq`, we aggregate the data based on both the date (`dteday`) and the hour segments (`hr_seg`. In case of dichotomous variables, we take the `mode` of the variable, with earlier defined function.
```{r aggregate hour, message=FALSE}
# Create a `mode` function for aggregation.
mode <- function(x) {
    which.max(table(x))
}

# Aggregate over `hr_bin` factor.
data <- data %>%
    group_by(dteday, hr_seg) %>%
    summarize(
        temp = mean(temp),
        atemp = mean(atemp),
        hum = mean(hum),
        windspeed = mean(atemp),
        weathersit = mode(weathersit),
        cnt = sum(cnt)
    )
```




# Model creation & comparison

Later on in [Assumptions](#assum) we concluded a non-linear (higher-order) relation between `hum` and `cnt`. To approximate a linear relation, we've transformed the data to `f(x) = x^3`, saved in column `hum3`.  

```{r solve non lin}
data$hum3 = data$hum^3
```

Create simple and complex models. Order is based on covariance table (high to low).
```{r model creation}
# Create models.
model1        <- lm(cnt ~ temp, data)
model2        <- lm(cnt ~ temp + hum3, data)
model3        <- lm(cnt ~ temp + hum3 + weathersit, data)
model4        <- lm(cnt ~ temp + hum3 + weathersit + windspeed, data)

# Include moderator for model of choice (#2), for comparison analysis.
model2_mod    <- lm(cnt ~ (temp + hum3)* hr_seg, data)
```

Comparison results table.
```{r comparison}
RMSE <- function(model) {
  model$residuals^2 %>% mean()
}

comp_df <- data.frame(
  model = c('model1', 'model2', 'model3', 'model4', 'model2_mod'),
  predictors = c('temp', 'temp + hum3', 'temp + hum3 + weathersit', 'temp + hum3 + weathersit + windspeed', 'temp + hum3'),
  moderator = c('', '', '', '', 'hr_seg'),
  AIC = c(AIC(model1), AIC(model2), AIC(model3), AIC(model4), AIC(model2_mod)),
  BIC = c(BIC(model1), BIC(model2), BIC(model3), BIC(model4), BIC(model2_mod)),
  RMSE = c(RMSE(model1), RMSE(model2), RMSE(model3), RMSE(model4), RMSE(model2_mod))
)


comp_df
```

Anova tests.
```{r anovas}
# Simple and complex model
anova(model1, model2)
anova(model2, model3)
anova(model3, model4)

# Test addition of moderator
anova(model2, model2_mod)
```

# Assumptions {#assum}

## Linearity {.tabset}

Small remark, the `car` library with `crPlots` function is not applicable on model with interactions, hence we plot per predictor to check relations.

### `temp`

```{r lin temp, message=FALSE, echo=FALSE}
ggplot(data, aes(temp, cnt, colour = hr_seg)) +
  geom_point() +
  geom_smooth()
```

The relationship is not quite linear. It seems that, when the temperature gets too high, people stop renting bikes. The higher temperatures which stop people from renting bikes, do not seem to occur in the night. Therefore, the relationship between temperature and rent count seems linear in the night.

### `hum`

```{r lin hum, message=FALSE, echo=FALSE}
ggplot(data, aes(hum, cnt, colour = hr_seg)) +
  geom_point() +
  geom_smooth()
```

To combat this non-linearity issue, we have transformed the predictor variable `hum` (as mentioned earlier). This does make the interpretation of the model less intuitive.

```{r lin hum3, message=FALSE, echo=FALSE}
ggplot(data, aes(hum3, cnt, colour = hr_seg)) +
  geom_point() +
  geom_smooth()
```

This is the best transformation we could come up with. It's not perfect, but we're accepting this as sufficiently linear.

## Full rank predictor matrix


The dataset contains 2190 observations (that is, the dataset after aggregating the data). It only contains 9 variables, including the transformed humidity variable. There are definitely more observations than there are variables.

Next, we need to know whether there is a relationship between the two predictors we are using, temperature and humidity.

```{r multicol, message=FALSE, echo=FALSE}
ggplot(data, aes(temp, hum3)) +
  geom_point() +
  geom_smooth()
```

This graph shows there is no relationship between temperature and humidity, so we can conclude there is no issue of multicollinearity.

## Exogenous predictors


We need to check whether there is no relationship between the predictors of the model, and its errors. 

```{r cov e}
cov_res <- cov(predict(model2_mod), resid(model2_mod))
cov_res
```

This value is practically equal to zero.

Next, we need to know whether the mean of the errors of the model are equal to zero.

```{r pred res relation, message=FALSE, echo=FALSE}
grid.arrange(
  ggplot(data, aes(temp, resid(model2_mod))) +  geom_point() +  geom_smooth(),
  ggplot(data, aes(hum, resid(model2_mod))) +  geom_point() +  geom_smooth()
)
```

The first graph is not quite right, though the line only starts deviating from 0, in the latter part of the graph. There's less data points there as well, so it makes sense. We can make do, though.
The second graph is completely fine.

## Constant & finite variance


We need to find out whether the variance of the errors is constant over all levels of the predictor (homoscedasticity). That is shown in the first plot below.
The second plot is similar, but it shows standardized residuals (or, more accurately, their square root).

```{r homosced}
# Residual plot.
model2_mod %>% plot(1)
model2_mod %>% plot(3)
```

The first plot is fine: the red line should be y = 0. That seems to be fine.
The second plot also seems to be roughly fine. There is a weird cluster of errors in the first part of both graphs; those are responsible for the "night" rentals.

## Independent errors

The errors should be independent. Now, we can check for clustering, though we have already manually clustered the data. We can check if that clustering was sufficient.

```{r}
ICCbare(data$temp, resid(model2_mod))
ICCbare(data$hum, resid(model2_mod))
ICCbare(data$hr_seg, resid(model2_mod))
```

All ICC are close enough to 0 to conclude that there is no issue of dependence of errors.

## Normally distributed errors


The errors should be normally distributed. We can make a Q-Q plot to check whether that's true.

```{r qqplot}
plot(model2_mod, 2)
```

We want the errors to be as close to the dotted line as possible. It's not looking to great, but it's okay enough.

## Influential data points


We need to check whether the model contains any influential data points. That includes outliers and high-leverage observations. We can check the outliers first.

```{r indep res}
plot(rstudent(model2_mod)) 
```

Again, the errors should be close to y = 0. And again, it doesn't look to great, but most of the errors seem to be close to y = 0. Interestingly, the earlier outlying values seem to be below 0, while the later values seem to be above 0. It may be possible that the bike renting program started at the first day of this dataset, so it became more popular over time. There is no way to be sure, though.

Next, we need to measure Cook's distance. Outliers are included in this measure as well, but since we checked those already, we really want to know about the high-leverage points.

```{r outliers post model}
plot(cooks.distance(model2_mod))            # Seems bad, but look at scale
plot(dfbetas(model2_mod)[,1])               # Seems good
```

Nowhere, Cook's distance is high at all. So this assumption seems to be sufficiently met.

# Interpretation of key results

## Model statistics and coefficients

In this section, we will check the quality of our linear model using the
R function `summary()` for our multiple linear regression model and the
same model including a categorical predictor variable. This new
categorical variable is named `hr_seg` and it works as our moderator,
counting the segments of the day in hours for the `night`, `morning-noon` and
`eve` (standing for evening).

```{r}
# Regression line model:check statistics and coefficients
summary(model2) 
```

**The summary outputs shows 6 components, including:**

-   Call: Shows the function call to compute our regression model, which
    is: $$
    lm(formula = cnt \~ (temp + hum3) \* hr_seg, data = data)
    $$
-   Residuals: A quick view of the distribution of the residuals.
-   Coefficients: Shows the regression beta coefficients and their
    statistical significance. Predictor variables, that are
    significantly associated with the outcome variable, appear marked by
    stars.
-   Residual Standard Error, R-squared, and the F-statistic check how
    well the model fits our data.

## Partial Effects - Multiple Linear Regression

### Interpretation

**What is the effect of humidity on average rental bike, after
controlling for temperature?**

First, the intercept tells us the location of the dependent variable,
when the independent variable equals 0 (\$x=0). Using our regression
model as an example, the expected *average* bike rental when temperature
reaches *0°C* is *659.44* units.

Second, the regression coefficients indicate the amount of change in the
predicted variable when the predictor increases with one unit. In this
regression model, the temperature is normalized, which is why the
*average* rental bike is expected to **increase** by *2974.36*, if humidity 
is assumed not to have any predictive power ("hum3"). 
Whereas, for each additional point of humidity, *average* 
bike rental is expected to *decrease by 2082.68 units*, if temperature
("temp") is assumed not to have any predictive power.

## Significance testing - P values

Probability values (p-values) measures the likelihood to find certain results 
(or more extreme results), assumed that the null hypothesis is true, 
as a fraction or decimal value between 0 and 1. 
**The closer the value is to 0, the stronger the evidence against the null hypothesis.**

This model has a p-value of *p \<2.2e-16*. Thus, **rejecting our null hypothesis**. We
conclude that there is a statistical significant relationship of temperature 
and humidity with the amount of bike rentals, due to our smaller p-values.

# Moderator variables

## Categorical predictors

```{r}
# Introducing new categorical variable:
# Check the coefficients and statistics results
summary(model2_mod)
```

**Little reminder:** The categorical variable `hr_seg` accounts for the
segments of hours in the categories `night`, `morning-noon` and `eve`(evening).

## Partial Effects - Categorical predictor variables

## Interpretation

In the output of our second model ("model 2 mod"), we see that the
categorical variable `hr_seg` is added to our multiple linear regression
model as a moderator. Therefore, testing the relationship of bike rental
and temperature to determine its statistical relationship.

**Temperature and humidity**

Compared to the multiple linear regression model, temperature remains
having an **average positive relationship** of *587.58 units*, meanwhile,
humidity showcases a **negative average relationship** of *94.16 units*,
though this effect is not significant.

**Intercept and reference groups**

-   `hr_seg` represents a segment of the day in hours and considers the
    reference group in this example, which is `night`.
-   Intercept represent the average bike rental for a bike rented in the
    `night`, assuming all other variables are 0.

**Categorical coefficients**

-   The **average** estimated value for renting a bike during the
    **morning-noon segment** of the day is of *961.65* units, **higher**
    than the **evening segment** of the day. In the evening
    segment of the day, the **average** estimated value for renting a
    bike is *548.62 units*, if all other variables are 0.

-   Including temperature and humidity in the model, the *average*
    estimated bike rental during the **morning-noon segment** is equal
    to *1627.07 units*, whereas the *average* estimated bike rental
    during the **evening** is *3076.57* units. Around **1449.5 bike rental units increased** 
    
## Model Fit

**How much bike rental variation is explained by the two models?**

## Comparison in statistics and coefficients

Comparing and contrasting the results in both models, **temperature**
and **humidity** variables remain having **consistent relationships**
for their average estimated values. **As temperature increases, humidity decreases**. 
Thus, cross validating our results by being consistent.

Overall, the **highest estimated value** belongs to the evening segment
of the model, introducing `hr_seg` as our categorical predictor
variable. It indicates an *average* bike rental *increase* during the
**evening segment with 3076.57 units**, after controlling for
temperature. Even **higher than** the overall coefficient values in
**our multiple linear regression model**.

Concerning the **second model** including `hr_seg` as the *moderator*, 
**humidity does not have a strong statistical relationship with bike rental** 
(*p = 0.372*). Certainly, higher than its p-value in the regression model.

Concerning the **p-value for temperature**, there is a slightly
increased probability of *p \< 3.91e-06* in the second model when
including the predictor variable `hr_seg`. Not alarming since it is
consistent toward 0, thus rejecting our null hypothesis.

# References

<div id="refs"></div>

