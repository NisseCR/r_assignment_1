---
title: "Bike Sharing"
author: "Aga, Karla, Nisse, Ole"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: united
    number_sections: false
    toc: true
    toc_float:
      collapsed: true
    df_print: paged
bibliography: references.bib  
---

# Abstract
![](../images/bike_sharing.jpeg)  

"*Bikesharing contributes to the advent of more sustainable transportation in cities around the globe.*" [@beland]. Bikesharing programs are designed to provide short-term bicycle rental in stations dispersed throughout the cities and located near public transportation hubs [@beland]. They have numerous environmental and health benefits, such as reducing congestion, complementing other forms of public transportation and encouraging exercise [@beland]. Additionally, accessible bike rental has a lower barrier of entry than purchasing your own bike and is more convenient for out-of-town commuters.  
In principle bicycles pose a good substitute for car use in urban areas, however, they have certain limitations. Among them is the exposure of cyclists to weather while commuting compared to other means of transportation. It would be intuitive for the number of bike rentals to be dependent on current weather conditions. If that is the case, the extent of that relationship would be important information for the bikesharing companies. Potentially, the information could be a factor in variety if business decisions, including level of pricing or supply for bikes in different seasons.   

---

Considering these possible applications, this report will attempt to answer: **To what extent can weather data predict the number of bike rentals in different parts of the day?** Weather data will includes temperature, wind speed, occurrence of weather phenomena (including storms, snow and rain), 

Operationalization of the RQ:  

* Control variables â€“ possibly a table with all of the variables.  

* Drawing of the path model.  

* Methodology.  

* Data description.  

To describe our data, we retrieved the hourly count of bike rentals between the years 2022 and 2012 from the machine learning repository UC Irvine. The metadata of this dataset belongs to Capital Bikeshare in Washington DC, United States. Thus, focusing on an American population with the corresponding weather and seasonal information.  


# References

<div id="refs"></div>

# Load data

Data set is acquired from a [machine-learning repository](https://archive.ics.uci.edu/dataset/275/bike+sharing+dataset), which provided us with three files, namely:  

* `hour.csv`, hourly data of bike rentals.
* `day.csv`, daily data of bike rentals.
* `README.txt`, providing additional metadata about the file and their data.

We first specify our dependencies and read the data from `hour.csv`. We also load `day.csv`, solely for comparison purposes in our [EDA](#eda) phase.  

```{r import, message = FALSE}
library(tidyverse)
library(fastDummies)
library(kableExtra)
library(gridExtra, exclude="combine")
library(lubridate)
```

```{r read bike}
# Source hourly data for model
source_data <- read.csv("../data/hour.csv", header = TRUE, sep = ",") %>%
  as_tibble()

# Extra daily data for EDA
day_df <- read.csv("../data/hour.csv", header = TRUE, sep = ",") %>%
  as_tibble()

# Clone source
data <- source_data

head(data)
```

We then create a sub-selection of variables that are of interest for our model; these are the control, predictor and outcome variables + variables necessary for EDA and preprocessing.

```{r select}
data <- select(data,
               dteday,
               hr, 
               weathersit, 
               temp, 
               atemp, 
               hum, 
               windspeed, 
               cnt)
```

TODO: perhaps describe columns, # of rows or the like. summarising dataframe?

# Preprocessing & EDA {#eda}

## Data Cleaning {.tabset}

### Expand `dteday` data

Expand `dteday` data by adding `dt_num` (numeric version of day, day_id so to speak).

```{r expand date}
# Converts datetime string to numeric with origin offset.
date_to_num <- function(date, offset) {
   as.integer(as.Date(date, "%Y-%m-%d")) - offset
}

# Fetch origin.
dt_offset <- source_data$dteday[1] %>%
  date_to_num(0) - 1


data <- data %>%
  mutate(
    dt_num = date_to_num(dteday, dt_offset)
  )

# Global variable for amount of unique days.
n_days <- n_distinct(data$dteday)
```

### Clustering of `hr` categories

Create clusters for `hr`. 24 dummy codes it too much.
```{r hour cut}
# Shift the `hr` data by 2 and increment the hour 23 for easier binning via the cut function.
data <- data %>%
  mutate(hr_idx = hr + 2)

data$hr_idx[data$hr == 23] <- 1

# Cut the 24 hour entries in 3 categories.
data <- data %>%
  mutate(hr_seg = cut(
    hr_idx,
    breaks = c(0, 8, 16, 24),
    labels = c("night", "morning-noon", "eve")
  ))
```

Check whether segmenting of `hr` is applied correctly by looking at single day data.

```{r bin check}
# Check output for single day.
data %>%
  filter(dteday == "2011-01-01") %>%
  arrange(hr) %>%
  select(hr, hr_seg) %>%
  head(24)
```

Segments were chosen based on `hr~cnt` data. Segments needed to be of equal size.

```{r determine segment, message=FALSE}
# Create a `mode` function for aggregation.
mode <- function(x) {
    which.max(table(x))
}

# Plot hr~cnt with current segmentation indicator.
data %>%
  group_by(hr, hr_seg) %>%
  summarise(
    cnt = sum(cnt)
  ) %>%
  ggplot(aes(hr, cnt, fill = hr_seg)) +
    geom_bar(stat = "identity")
```

## The rest

Summary of our data.

```{r summary}
summary(data)
```

Data contains `r n_days` unique days, meaning that the entirety of 2011 (365 days) and 2012 (366 days) was recorded.

## Distributions

Distributions of variables.

```{r distributions, message = FALSE, echo = FALSE}
grid.arrange(
    ggplot(data, aes(hr)) + geom_histogram(binwidth = 1),
    ggplot(data, aes(weathersit)) + geom_histogram(binwidth = 1),
    ggplot(data, aes(temp)) + geom_histogram(),
    ggplot(data, aes(atemp)) + geom_histogram(),
    ggplot(data, aes(hum)) + geom_histogram(),
    ggplot(data, aes(windspeed)) + geom_histogram(),
    ggplot(data, aes(cnt)) + geom_histogram()
)
```
Notice the following:

-   Missing records in some `hr` categories.
-   Occurrence gap in `windspeed` distribution.
-   Category 4 of `weathersit` is barely present.

## Outliers

Outlier detection using quantiles.  
```{r outliers}
grid.arrange(
    ggplot(data, aes(temp)) + geom_boxplot(),
    ggplot(data, aes(atemp)) + geom_boxplot(),
    ggplot(data, aes(hum)) + geom_boxplot(),
    ggplot(data, aes(windspeed)) + geom_boxplot(),
    ggplot(data, aes(cnt)) + geom_boxplot()
)
```

Notice the following:

-   Large amount of outliers in `cnt` (outcome variables).
-   Some outliers in `windspeed`.
-   3x `0.00` values of `hum`.

Large amount of `cnt` outliers is due to low rental count during the night, which shifts the median to a lower value. `hr_seg` acts as moderator, so better to check data displays *per* `hr_seg` cluster:

```{r outliers per hr seg}
ggplot(data, aes(cnt, colour = hr_seg)) + geom_boxplot()
```

## Missing data {.tabset}

Check standard NA values. There are none.

```{r missing standard}
anyNA(data)
```

### Missing records for hour

Some `hr` values seem to have a lower amount of occurrences than others.

```{r hr missing}
hr_df <- data %>%
  group_by(hr) %>%
  summarise(
    n = n(),
    n_missing_days = n_days - n)

ggplot(hr_df, aes(hr, n_missing_days)) +
  geom_bar(stat = "identity")
```

Determine where missing records are located. Seems that a system outtage (or the like) was present on `2012-10-29` and `2012-10-30`, with a combined total of `35` missing records.

```{r missing location, message = FALSE}
missing_df <- data %>%
  group_by(dteday, dt_num) %>%
  summarise(
    n_records = n(),
    n_missing_records = 24 - n_records
  ) %>%
  filter(n_missing_records > 0) %>%
  arrange(desc(n_missing_records)) %>%
  select(dteday, n_missing_records)

head(missing_df, 10)
```

### Gap in windspeed distribution

Might have to do with minimal threshold of the sensor to measure windspeed.

## Data Aggregation
After having created `hr_seq`, we aggregate the data based on both the date (`dteday`) and the hour segments (`hr_seg`. In case of dichotomous variables, we take the `mode` of the variable.
```{r aggregate hour}
# Aggregate over `hr_bin` factor.
data <- data %>%
    group_by(dteday, hr_seg) %>%
    summarize(
        temp = mean(temp),
        atemp = mean(atemp),
        hum = mean(hum),
        windspeed = mean(atemp),
        weathersit = mode(weathersit),
        cnt = sum(cnt)
    )
```

# Linear model

```{r model}
fit <- lm(cnt ~ temp + hum, data)
```